\chapter{Related Work}
Although the widest usage of the MapReduce programming model to date may be the one of Hadoop \cite{Hadoop} with its centralised master-slave architecture, recently, there have also been a number of attempts to transport it to a more decentralised setting in an endeavour to support currently popular Cloud platforms, pervasive grids, and/or mobile environments. 

\cite{Marozzo2012a} recognised the problem of centralised master-slave architectures to not cope well with dynamic Cloud infrastructures, where nodes may join or leave the network at high rates. Thus, they introduce a peer-to-peer model to manage node churn but also master failures and job recovery in a decentralised way. Each node may become a master or a slave at any given time dynamically, preserving a certain master/slave ratio. Slaves are assigned tasks to perform by the masters, which handle management, recovery, and coordination. To reduce job loss in case of master failures, each master may act as a backup master for a certain job, only executing it if the master primarily responsible for that job fails. Overall, the structure of different entities resembles very much the one of Hadoop, simply ported to a P2P setting. They were able to show that such an implementation provides better fault tolerance levels compared to a centralised implementations of MapReduce and only limited impact on network overhead. 

In a very similar direction goes the idea of CloudFIT \cite{Steffenel2015}, which the authors advertise as a "Platform-as-a-Service (PaaS) middleware that allows the creation of private clouds over pervasive environments". The idea is to use private computers to set up a private "Cloud", and that MapReduce jobs are then distributed to this environment and executed in a P2P fashion. CloudFIT is, thus, built to support various P2P overlays and the authors show the performance of CloudFIT with both PAST and TomP2P against Hadoop. The results demonstrated CloudFIT to be able to achieve similar execution speeds as Hadoop while omitting the need of a dedicated cluster of computers. Data in CloudFIT is directly stored within the DHT of the corresponding overlay, such that it is replicated by a factor of k. Although such an implementation may not guarantee n-resiliency (meaning, replicating the data on each node), it is a reasonable assumption that fault tolerance may be ensured more than enough (provided at least a number of nodes stay alive) while drastically improving storage performance. Furthermore, not all the data is broadcasted but only the keys of each task, further reducing the network usage the authors showed in another study \cite{Steffenel2015a} to be one of the main problems in distributed environments for the performance of MapReduce tasks.

